{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d642d5b4",
   "metadata": {},
   "source": [
    "Code to load CK's extracted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c32f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (39426, 28, 28, 1)\n",
      "Training labels shape: (39426,)\n",
      "Test data shape: (9988, 28, 28, 1)\n",
      "Test labels shape: (9988,)\n",
      "Number of classes: 36\n",
      "Class names: ['0' '1' '2' '3' '4' '5' '6' '7' '8' '9' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h'\n",
      " 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def load_images_from_folder(folder_path, target_size, color_mode):\n",
    "    images = []\n",
    "    labels = []\n",
    "    valid_extensions = ['.jpg', '.jpeg', '.JPG', '.JPEG','.png']\n",
    "    channels = 1 if color_mode == 'grayscale' else 3\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if any(filename.endswith(ext) for ext in valid_extensions):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                # Read image\n",
    "                if color_mode == 'grayscale':\n",
    "                    image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "                else:\n",
    "                    image = cv2.imread(file_path)\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                \n",
    "                # Resize and normalize\n",
    "                image = cv2.resize(image, target_size)\n",
    "                image = image.astype('float32') / 255.0\n",
    "                \n",
    "                # Add channel dimension if grayscale\n",
    "                if channels == 1:\n",
    "                    image = np.expand_dims(image, axis=-1)\n",
    "                \n",
    "                # Store image and label\n",
    "                images.append(image)\n",
    "                labels.append(os.path.splitext(filename)[0].split('-')[0])\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {filename}: {str(e)}\")\n",
    "                continue\n",
    "                \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "def prepare_fixed_split(train_dir, test_dir, target_size=(28, 28), color_mode='grayscale'):\n",
    "\n",
    "    # Load training data\n",
    "    X_train, train_labels = load_images_from_folder(train_dir, target_size, color_mode)\n",
    "    \n",
    "    # Load test data\n",
    "    X_test, test_labels = load_images_from_folder(test_dir, target_size, color_mode)\n",
    "    \n",
    "    # Combine labels to ensure consistent encoding\n",
    "    all_labels = np.concatenate([train_labels, test_labels])\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(all_labels)\n",
    "    \n",
    "    # Encode both sets of labels\n",
    "    y_train = label_encoder.transform(train_labels)\n",
    "    y_test = label_encoder.transform(test_labels)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, label_encoder\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_dir = \"train_extracted\"\n",
    "    test_dir = \"test_extracted\"\n",
    "    target_size = (28, 28) \n",
    "    color_mode = 'grayscale' \n",
    "\n",
    "    # Load and prepare data\n",
    "    X_train, X_test, Y_train, Y_test, label_encoder = prepare_fixed_split(\n",
    "        train_dir, test_dir, target_size, color_mode\n",
    "    )\n",
    "\n",
    "    # Verify shapes and classes\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Training labels shape: {y_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "    print(f\"Test labels shape: {y_test.shape}\")\n",
    "    print(f\"Number of classes: {len(label_encoder.classes_)}\")\n",
    "    print(f\"Class names: {label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356db08f",
   "metadata": {},
   "source": [
    "Code to load Pooja's extracted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8913959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (42136, 28, 28, 1)\n",
      "Training labels shape: (42136,)\n",
      "Test data shape: (9021, 28, 28, 1)\n",
      "Test labels shape: (9021,)\n",
      "Number of classes: 36\n",
      "Class names: ['0' '1' '2' '3' '4' '5' '6' '7' '8' '9' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h'\n",
      " 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
     ]
    }
   ],
   "source": [
    "# Code to load Pooja's extracted characters\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def load_images_from_folder(folder_path, target_size, color_mode):\n",
    "    images = []\n",
    "    labels = []\n",
    "    valid_extensions = ['.jpg', '.jpeg', '.JPG', '.JPEG','.png']\n",
    "    channels = 1 if color_mode == 'grayscale' else 3\n",
    "\n",
    "    for label in os.listdir(folder_path):\n",
    "        sub_folder_path = os.path.join(folder_path, label)\n",
    "        for filename in os.listdir(sub_folder_path):\n",
    "            if any(filename.endswith(ext) for ext in valid_extensions):\n",
    "                file_path = os.path.join(sub_folder_path, filename)\n",
    "                try:\n",
    "                    # Read image\n",
    "                    if color_mode == 'grayscale':\n",
    "                        image = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
    "                    else:\n",
    "                        image = cv2.imread(file_path)\n",
    "                        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "                    \n",
    "                    # Resize and normalize\n",
    "                    image = cv2.resize(image, target_size)\n",
    "                    image = image.astype('float32') / 255.0\n",
    "                    \n",
    "                    # Add channel dimension if grayscale\n",
    "                    if channels == 1:\n",
    "                        image = np.expand_dims(image, axis=-1)\n",
    "                    \n",
    "                    # Store image and label\n",
    "                    images.append(image)\n",
    "                    labels.append(label)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {filename}: {str(e)}\")\n",
    "                    continue\n",
    "                \n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "def prepare_fixed_split(train_dir, test_dir, target_size=(28, 28), color_mode='grayscale'):\n",
    "\n",
    "    # Load training data\n",
    "    X_train, train_labels = load_images_from_folder(train_dir, target_size, color_mode)\n",
    "    \n",
    "    # Load test data\n",
    "    X_test, test_labels = load_images_from_folder(test_dir, target_size, color_mode)\n",
    "    \n",
    "    # Combine labels to ensure consistent encoding\n",
    "    all_labels = np.concatenate([train_labels, test_labels])\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(all_labels)\n",
    "    \n",
    "    # Encode both sets of labels\n",
    "    y_train = label_encoder.transform(train_labels)\n",
    "    y_test = label_encoder.transform(test_labels)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, label_encoder\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_dir = \"output-train-by-char\"\n",
    "    test_dir = \"output-test\"\n",
    "    target_size = (28, 28) \n",
    "    color_mode = 'grayscale' \n",
    "\n",
    "    # Load and prepare data\n",
    "    X_train, X_test, Y_train, Y_test, label_encoder = prepare_fixed_split(\n",
    "        train_dir, test_dir, target_size, color_mode\n",
    "    )\n",
    "\n",
    "    # Verify shapes and classes\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Training labels shape: {Y_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "    print(f\"Test labels shape: {Y_test.shape}\")\n",
    "    print(f\"Number of classes: {len(label_encoder.classes_)}\")\n",
    "    print(f\"Class names: {label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef8a571",
   "metadata": {},
   "source": [
    "Check loaded data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10ed13a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (42136, 28, 28, 1)\n",
      "Training labels shape: (42136,)\n",
      "Test data shape: (9021, 28, 28, 1)\n",
      "Test labels shape: (9021,)\n",
      "Number of classes: 36\n",
      "Class names: ['0' '1' '2' '3' '4' '5' '6' '7' '8' '9' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h'\n",
      " 'i' 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training data shape: {X_train.shape}\")\n",
    "print(f\"Training labels shape: {Y_train.shape}\")\n",
    "print(f\"Test data shape: {X_test.shape}\")\n",
    "print(f\"Test labels shape: {Y_test.shape}\")\n",
    "print(f\"Number of classes: {len(label_encoder.classes_)}\")\n",
    "print(f\"Class names: {label_encoder.classes_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eae4168",
   "metadata": {},
   "source": [
    "Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c14d386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create new conda env for this. Default one having issue with keras\n",
    "\n",
    "from tf_keras.models import Sequential\n",
    "from tf_keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tf_keras.preprocessing.image import ImageDataGenerator\n",
    "from tf_keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "\n",
    "# Shuffle data before split so that the classes are not completely absent from either side\n",
    "X_train, Y_train = shuffle(X_train, Y_train, random_state=42)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,      # ±15 degree rotation\n",
    "    width_shift_range=0.1,  # 10% horizontal shift\n",
    "    height_shift_range=0.1, # 10% vertical shift\n",
    "    zoom_range=0.1,         # 10% zoom\n",
    "    shear_range=0.1,        # 10% shear\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "# Use flow_from_dataframe if you have structured data\n",
    "train_generator = train_datagen.flow(\n",
    "    X_train, Y_train,\n",
    "    batch_size=32,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow(\n",
    "    X_train, Y_train,\n",
    "    batch_size=32,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Classifier\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "input_shape = X_train.shape[1:]\n",
    "model = create_model(input_shape=input_shape, num_classes=num_classes)\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=3)\n",
    "]\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "                    epochs=50,\n",
    "                    batch_size=32,\n",
    "                    validation_data=val_generator)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=2)\n",
    "print(f\"\\nTest accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(Y_test, y_pred_classes, \n",
    "                           target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "plt.figure(figsize=(10,8))\n",
    "cm = confusion_matrix(Y_test, y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "model.save('character_recognition_model.keras')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6872d4",
   "metadata": {},
   "source": [
    "Pretrained CNN model on EMNIST before fine tuning on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe74808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_9 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPoolin  (None, 13, 13, 32)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 11, 11, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPoolin  (None, 5, 5, 64)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 3, 3, 128)         73856     \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 1152)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 128)               147584    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 36)                4644      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 244900 (956.64 KB)\n",
      "Trainable params: 244900 (956.64 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "5453/5453 [==============================] - 124s 22ms/step - loss: 0.3685 - accuracy: 0.8708 - val_loss: 0.2742 - val_accuracy: 0.8977\n",
      "Epoch 2/20\n",
      "5453/5453 [==============================] - 117s 21ms/step - loss: 0.2585 - accuracy: 0.9028 - val_loss: 0.2486 - val_accuracy: 0.9061\n",
      "Epoch 3/20\n",
      "5453/5453 [==============================] - 118s 22ms/step - loss: 0.2373 - accuracy: 0.9094 - val_loss: 0.2416 - val_accuracy: 0.9082\n",
      "Epoch 4/20\n",
      "5453/5453 [==============================] - 118s 22ms/step - loss: 0.2240 - accuracy: 0.9137 - val_loss: 0.2471 - val_accuracy: 0.9069\n",
      "Epoch 5/20\n",
      "5453/5453 [==============================] - 117s 22ms/step - loss: 0.2144 - accuracy: 0.9162 - val_loss: 0.2389 - val_accuracy: 0.9098\n",
      "Epoch 6/20\n",
      "5453/5453 [==============================] - 117s 21ms/step - loss: 0.2066 - accuracy: 0.9182 - val_loss: 0.2388 - val_accuracy: 0.9113\n",
      "Epoch 7/20\n",
      "5453/5453 [==============================] - 117s 21ms/step - loss: 0.1994 - accuracy: 0.9204 - val_loss: 0.2388 - val_accuracy: 0.9108\n",
      "Epoch 8/20\n",
      "5453/5453 [==============================] - 118s 22ms/step - loss: 0.1936 - accuracy: 0.9222 - val_loss: 0.2419 - val_accuracy: 0.9101\n",
      "Epoch 9/20\n",
      "5453/5453 [==============================] - 117s 21ms/step - loss: 0.1885 - accuracy: 0.9239 - val_loss: 0.2484 - val_accuracy: 0.9091\n",
      "Epoch 10/20\n",
      "5453/5453 [==============================] - 114s 21ms/step - loss: 0.1833 - accuracy: 0.9255 - val_loss: 0.2518 - val_accuracy: 0.9097\n",
      "Epoch 11/20\n",
      "5453/5453 [==============================] - 7968s 1s/step - loss: 0.1792 - accuracy: 0.9266 - val_loss: 0.2619 - val_accuracy: 0.9086\n",
      "Epoch 12/20\n",
      "5452/5453 [============================>.] - ETA: 0s - loss: 0.1754 - accuracy: 0.9278"
     ]
    }
   ],
   "source": [
    "# TODO: Create new conda env for this. Default one having issue with keras\n",
    "from tf_keras.models import Sequential\n",
    "from tf_keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tf_keras.preprocessing.image import ImageDataGenerator\n",
    "from tf_keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tf_keras.utils import to_categorical\n",
    "from tf_keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sns\n",
    "\n",
    "def create_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(64, (3, 3), activation='relu'),\n",
    "        MaxPooling2D((2, 2)),\n",
    "        Conv2D(128, (3, 3), activation='relu'),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=1e-3),\n",
    "                  loss='sparse_categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "\n",
    "num_classes = len(label_encoder.classes_)\n",
    "input_shape = X_train.shape[1:]\n",
    "model = create_model(input_shape=input_shape, num_classes=num_classes)\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(factor=0.1, patience=3)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_emnist_from_csv(csv_path):\n",
    "    \"\"\"Load EMNIST data from CSV file\"\"\"\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "    labels = df.iloc[:, 0].values\n",
    "    images = df.iloc[:, 1:].values\n",
    "    return images, labels\n",
    "\n",
    "def preprocess_emnist(images, labels):\n",
    "    \"\"\"Preprocess EMNIST images and labels\"\"\"\n",
    "    # Reshape to 28x28 and transpose (EMNIST images are rotated)\n",
    "    images = images.reshape((-1, 28, 28)).transpose(0, 2, 1)\n",
    "    \n",
    "    # Flip images to correct orientation\n",
    "    # images = np.array([np.fliplr(img) for img in images])\n",
    "    \n",
    "    # Normalize pixel values\n",
    "    images = images.astype('float32') / 255.0\n",
    "    \n",
    "    # Add channel dimension (for CNN input)\n",
    "    images = np.expand_dims(images, axis=-1)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "def create_label_mapping():\n",
    "    # For merged case (36 classes: 0-9 + A-Z)\n",
    "    label_names = [str(i) for i in range(10)]  # Digits 0-9\n",
    "    label_names += [chr(65 + i) for i in range(26)]  # A-Z\n",
    "    label_names += [chr(97 + i) for i in range(26)]  # a-z\n",
    "    return label_names\n",
    "\n",
    "\n",
    "# Create mapping (assuming you've merged cases as before)\n",
    "label_mapping = create_label_mapping()  \n",
    "\n",
    "# Example usage with your data\n",
    "def convert_labels(y_numerical):\n",
    "    \"\"\"Convert numerical labels to character labels\"\"\"\n",
    "    return [label_mapping[label] for label in y_numerical]\n",
    "\n",
    "def merge_case_labels(y_original):\n",
    "    \"\"\"Convert labels so uppercase and lowercase of same letter have same class\"\"\"\n",
    "    y_new = y_original.copy()\n",
    "    \n",
    "    # Digits 0-9 remain unchanged (labels 0-9)\n",
    "    # Letters A-Z and a-z need to be merged\n",
    "    \n",
    "    # For uppercase letters (A-Z is 10-35 in original labels)\n",
    "    upper_case = (y_original >= 10) & (y_original <= 35)\n",
    "    # Convert to 10-35 (same as original)\n",
    "    \n",
    "    # For lowercase letters (a-z is 36-61 in original labels)\n",
    "    lower_case = (y_original >= 36) & (y_original <= 61)\n",
    "    # Convert to 10-35 (matching their uppercase counterparts)\n",
    "    y_new[upper_case] = y_new[upper_case] + 26\n",
    "    \n",
    "    return y_new\n",
    "\n",
    "\n",
    "# Load training data\n",
    "train_images, train_labels = load_emnist_from_csv('emnist-byclass-train.csv')\n",
    "train_images, train_labels = preprocess_emnist(train_images, train_labels)\n",
    "\n",
    "# Load test data\n",
    "test_images, test_labels = load_emnist_from_csv('emnist-byclass-test.csv')\n",
    "test_images, test_labels = preprocess_emnist(test_images, test_labels)\n",
    "\n",
    "# Apply to both train and test sets\n",
    "train_labels = np.array(merge_case_labels(train_labels))\n",
    "test_labels = np.array(merge_case_labels(test_labels))\n",
    "\n",
    "#Convert labels\n",
    "train_labels = np.array(convert_labels(train_labels))\n",
    "test_labels = np.array(convert_labels(test_labels))\n",
    "\n",
    "train_labels = label_encoder.transform(train_labels)\n",
    "test_labels = label_encoder.transform(test_labels)\n",
    "\n",
    "# Now we have 36 classes (0-9 digits + 26 letters)\n",
    "num_classes = 36\n",
    "\n",
    "# Create and train model (using num_classes=36 now)\n",
    "model.fit(train_images, train_labels,\n",
    "          epochs=20,\n",
    "          batch_size=128,\n",
    "          validation_data=(test_images, test_labels))\n",
    "\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=2)\n",
    "print(f\"\\nTest accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(Y_test, y_pred_classes, \n",
    "                           target_names=label_encoder.classes_))\n",
    "\n",
    "\n",
    "\n",
    "# Shuffle data before split so that the classes are not completely absent from either side\n",
    "X_train, Y_train = shuffle(X_train, Y_train, random_state=42)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=15,      # ±15 degree rotation\n",
    "    width_shift_range=0.1,  # 10% horizontal shift\n",
    "    height_shift_range=0.1, # 10% vertical shift\n",
    "    zoom_range=0.1,         # 10% zoom\n",
    "    shear_range=0.1,        # 10% shear\n",
    "    validation_split=0.2,\n",
    ")\n",
    "\n",
    "# Use flow_from_dataframe if you have structured data\n",
    "train_generator = train_datagen.flow(\n",
    "    X_train, Y_train,\n",
    "    batch_size=32,\n",
    "    subset=\"training\"\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow(\n",
    "    X_train, Y_train,\n",
    "    batch_size=32,\n",
    "    subset=\"validation\"\n",
    ")\n",
    "\n",
    "# 1. Freeze Convolutional Base\n",
    "for layer in model.layers[:4]:  # First two conv-pool blocks\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[4:]:  # Last conv block and dense layers\n",
    "    layer.trainable = True\n",
    "\n",
    "# # 2. Modify the Top Layers\n",
    "# # Remove original classification head (last 2 layers)\n",
    "# model.pop()  # Remove output layer\n",
    "# model.pop()  # Remove dense layer\n",
    "\n",
    "# # Add new dense layers for custom dataset\n",
    "# model.add(Dense(64, activation='relu', name='new_dense'))\n",
    "# model.add(Dense(num_classes, activation='softmax', name='new_output'))\n",
    "\n",
    "# 3. Recompile with Lower Learning Rate\n",
    "model.compile(optimizer=Adam(learning_rate=1e-4),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# 4. Display Trainable Layers\n",
    "for i, layer in enumerate(model.layers):\n",
    "    print(f\"Layer {i}: {layer.name} - Trainable: {layer.trainable}\")\n",
    "\n",
    "# 5. Fine-Tune on Custom Data\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=val_generator,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "\n",
    "# history = model.fit(train_generator,\n",
    "#                     epochs=50,\n",
    "#                     batch_size=32,\n",
    "#                     validation_data=val_generator)\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=2)\n",
    "print(f\"\\nTest accuracy: {test_acc:.4f}\")\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "\n",
    "# Generate predictions\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(Y_test, y_pred_classes, \n",
    "                           target_names=label_encoder.classes_))\n",
    "\n",
    "# Confusion matrix\n",
    "plt.figure(figsize=(10,8))\n",
    "cm = confusion_matrix(Y_test, y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Save the model\n",
    "model.save('pretrained_character_recognition_model.keras')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
